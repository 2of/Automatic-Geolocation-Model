# City-scale Geolocation from Street-Level Images using Attention Nets, Object Localization, Colour-Space embeddings and Text Extraction
This is the repo for the capstone MAI project for the University of Canterbury 2024-2025. Essentially, we're making a glorified GeoGuessr bot... More explained in the business section. This doc was mostly Generated by MS Copilot, based on information from the Report and Report proposal (both of which are in /reports)



## Geolocalization ? What is that? Locating Rocks?

This repo contains the tools, code, model and a small sample of training data (obfuscated) for building the model, as well as evaluative tools and results information. 



<p align="center">
  <img src="./docs/gitembeds/header1.png" width="30%" />
  <img src="./docs/gitembeds/arrowright.png" width="10%" style="padding: 0 20px; vertical-align: ;" />
  <img src="./docs/gitembeds/springfield_w_point.jpg" width="30%" />
</p>





## How and Why People Play GeoGuessr

GeoGuessr is an online geography game that challenges players to identify locations based on Google Street View images. You are dropped into a random location somewhere in the world, and your task is to figure out where you are. The cognintive process for making a guess is typically one where the player recursively shrinks their candidate location pool (as a classificiation or a regression) based on decreasingly verbose image features. I.e. they *narrow down* the places they could be!

### The Rough Process of Making a Guess:

1. **Start the Game**: You're placed somewhere in the world with no context.
2. **Analyze Clues**:
   - **Road Signs**: Look for street signs, language, and text. Often, the language will give away the continent or country.
   - **Landmarks**: Recognizable buildings, monuments, or natural formations can help you pinpoint your location.
   - **Nature**: The type of vegetation or climate (tropical forests, deserts, mountains) can give you clues about your region.
   - **Road Features**: Observing things like road markings, vehicle types, or even which side of the road people are driving on can help narrow it down.
3. **Make Your Guess**: Drag a pin on the world map to where you think the location is. The closer your guess, the higher your score.
4. **Compare Your Score**: Each round gives you a score based on how close you are to the actual location. The goal is to get as many points as possible across multiple rounds.

### People can be *absurdly* good at GeoGuessr

<p style="text-align: center;">
  <a href="https://www.youtube.com/watch?v=PeXvpxPj1mo" target="_blank" style="position: relative; display: inline-block;">
    <img src="https://img.youtube.com/vi/PeXvpxPj1mo/hqdefault.jpg" alt="Watch this video" style="border: 3px solid #ddd; border-radius: 8px; width: 200px; box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);">
  </a>
</p>

### and it turns out, so can deep learning (although with more electricity and less caffeine) 

## What's in the repo?
Everything you need to build out your own GeoInformer is in here (or will be added) you'll need to BYO API key for google streetview. A few images are in /data for the purpose of validation


### Tools:
 1. A handy GUI tool for downloading google street view data (BYO api key) using QT5
<p align="center">
  <img src="./docs/gitembeds/tool_dler_gui.png" width="50%" />
</p>

 2. A handy tool to generate evenly distributed geolocation data for streetview images (as even as we can be given the obvious topologicial challenges
 (not shown as it's commandline, but here's the gist)
<p align="center">
  <img src="./docs/gitembeds/extraction_of_points.png" width="50%" />
</p>


# Background and Some Motivation
## Introduction

Most image formats contain geolocation information in their metadata or headers. When this information is absent, we propose that geolocation can be determined with useful accuracy from latent features derived directly from the image data itself. These features, referred to as Geo Informative Features, inform a deep learning model to determine latitude and longitude coordinates. Our data is sourced primarily from the Google Street View API.

The popular online video game GeoGuessr gamifies this process for humans. Players are presented with a randomized Google Street View location and score based on the topological distance between their selected location on a map and the actual capture location. This approach translates well to a machine learning model.

Some Geo Informative features are highly meaningful as they significantly reduce the candidate pool or are uniquely identifying (e.g., a sign reading "Carter’s Road Grocer, Carterton’s Top Grocer!" or a recognizable landmark like the Eiffel Tower). The most telling features are often derived from text, such as phone numbers and web domains.

![Simpsons Meme Placeholder](/docs/gitembeds/moespetshop.webp)


## Approach

We leverage several advanced techniques:

- **Object Detection**: Utilizing YOLO for efficient real-time object detection.
- **Text Extraction**: Using docTR for robust text extraction.
- **Color Extraction**: Implementing custom algorithms for HSV color histogram analysis.

These extracted features form a combined embedding. The embedding is then passed through a multihead attention layer, enhancing the model’s ability to learn complex patterns by selectively focusing on the most relevant parts of the combined embeddings. The architecture combines sparse features to form a dense embedding matrix.


Essentially, we want to concatenate **sparse** embeddings into a **dense** embedding, hereby creating something we call a **GeoLocalization Pattern**



![Simpsons Meme Placeholder](/docs/gitembeds/clowncar.gif)

Density is good, for this model

## Querying Candidate Areas within 10km

Each point within the defined 10km space is systematically evaluated for potential inclusion in the training dataset, with hardware constraints determining viable candidates.

*Figure 3: Example of the 10km area evaluated for query points. Blue dots represent plausible points to query for data collection.*

The identified candidate areas for queries are shown below. To reduce API calls, candidates only include those intersecting viable StreetView locations (i.e., streets and roads) in the OpenStreetMap Dataset. The relative spacing \(d\) for each point considers the StreetView capture spacing, ranging from a few meters to 20 meters. Setting \(d = 8\) will sufficiently capture the necessary datapoints.

*Figure 4: Example of points reduced to candidate locations and the result of querying the GoogleMaps API (third image generated using Microsoft Copilot 14/11/2024).*

To further refine the pool of candidate areas, we apply filtering based on OpenStreetMap data using ShapeFiles, reducing the number of calls to the StreetView API. This results in a reduction of 50-90% of candidate points.

After querying the Google Street View API, the dataset comprises a yet unknown number of three-channel images, each with a resolution of 640x640. These images are categorized by their geographic coordinates (latitude and longitude) according to the query locations returned by the StreetView API, which snaps a query to the nearest LAT / LONG. Successful queries update the LAT / LONG to the new truth LAT / LONG for that data point. For API cost minimization and noise reduction, queries within distance \(d\) are no longer candidates unless already queried.

## Recency

Where multiple Google StreetView Images are available at differing times for the same location, all images become part of the dataset.

## Preprocessing and Data

This GeoLocator is scoped to Chicago for computability, defining a 10km radius around the center point of Chicago.

We define the basic preprocessing pipeline as follows; noting that Google StreetView snaps queries to the nearest available capture.

*Figure 5: PreProcessing Overview.*

From the coordinates within this radius, we define a pool of training locations to query through the Google Street View API. Considering distances between StreetView imagery range from a few meters to a dozen or so, duplication of images is undesirable in the training set, and missing images are inconsequential.

The volume of training points for a given radius is calculated as follows with integer rounding:

$$NumQueries = \frac{\pi \times R^2}{d^2}$$

Where \(d\) is the distance (meters) between each query point. The scaling of total candidate points about a radius is exponential. We define \(d = 10\), noting that StreetView images occupy some proportion of this location. The density of StreetView imagery is inconsistent across locales.

A critical notion in the test data is that local features used as geolocators should be present in multiple images. Defining \(d\) this way is critical to achieving good generalization of the model.

| Radius (km) | Number of Points (N) |
|-------------|-----------------------|
| 1           | 31,416                |
| 5           | 785,398               |
| 10          | 3,141,593             |
| 11          | 3,801,327             |
| 12          | 4,523,893             |

While we select \(d = 10\), the scaling for increasingly large \(d\) is evident above.

## Method

Our goal is to define GeoInformers and construct a model which, given GeoInformers, can predict the location of an unseen image at city scale. GeoInformers are determined from a branched path, and their embeddings are used as inputs for further neural network layers.

### Street Level Imagery

Consider the following image, randomly chosen in Chicago. The image intuitively contains several useful GeoInformers.

*Figure 6: A random location in Chicago, text is a significant identifier (screencapture from MAPS public website).*

### Pipeline

The overall structure of our implementation is depicted below. The final model architecture is still being determined. The model has three branches:

- **Color Evaluation**:
  - Generates a histogram for the HSV color space.
  - Produces a corresponding color embedding.

- **OCR Text Extraction**:
  - Utilizes an OCR model to extract text (docTR).
  - Creates a text embedding from the extracted textual information.

- **Object Detection**:
  - Employs an object detection model trained on a manually selected pool of plausible objects.
  - Generates object embeddings for the detected items.

An attention layer then processes the combined embeddings:
- Dynamically weighs the importance of each feature.
- Enhances the overall geoinformative power of the model.

*Figure 7: Overall structure of the implementation pipeline.*

### Architecture and Activation Functions

Our model processes 640 × 640 input images using YOLOv11 for object detection and docTR for text recognition.

The input tensor size is (B, 640, 640, 3).

YOLOv11 outputs a tensor (B, N, 4 + 1 + C), where N is the number of detections per image, 4 represents the bounding box coordinates [x1, x2, y1, y2], 1 is the objectness score (Confidence), and C is the number of class probabilities.

docTR outputs a tensor (B, T, d), where T is the number of text instances detected, and d is the dimension of the text features.

The YOLOv11 embeddings are reshaped to (B, N, Ey) and the docTR embeddings to (B, T, Ed).

The combined tensor size before the attention model is (B, N + T, E), where E is the unified embedding dimension.

Given that the model exceeds 7 layers, we use activation functions like ReLU or Leaky ReLU to mitigate gradient loss, ensuring effective learning.

### Color Histogram Embedding

The overall color space of an image is a telling geolocator that permeates across input variables. The roofline in Figure 9, for example, is a telling geolocator, yet the sky is consistent but is not a useful feature for our model.

This branch of the model is essentially preprocessing only; we create color histograms representing the distributions of the local colors.

We convert the image to the HSV color space to better capture color variations and compute a histogram for each channel (Hue, Saturation, and Value). These histograms are then flattened and normalized to create the color vector.

The extracted histogram is used as an input feature in a separate branch of the neural network. This branch processes the histogram through several dense layers to identify significant color patterns. The output from this branch is concatenated with the object embeddings and the text embeddings before the attention layer(s).

*Figure 8: Example of HSV extraction from Figure 9 and a sample embedding (truncated).*

### Text Extraction and Processing

Text is a significant geolocator. We employ an off-the-shelf solution: the docTR OCR model. No hyperparameter tuning or model refinement is applied to this branch of the model. Images are normalized before feeding into the model.

Each training set member undergoes processing via the docTR model, which yields an object containing the localized text. While docTR may not capture every piece of text flawlessly, any errors introduced are minor and do not significantly impact the performance of the greater model.

Following






#### Models:
 1. Todo

#### src:
 1. Todo

#### docs:
 1. Todo





#### frontend:
 1. React frontend for uploading images to the model (rather to a rest API hosted on code you'll find in /server/)
 2. Some fun and fancy css!

