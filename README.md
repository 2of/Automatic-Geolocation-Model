# City-scale Geolocation from Street-Level Images using Attention Nets, Object Localization, Colour-Space Embeddings, and Text Extraction

This is the repo for the capstone MAI project for the University of Canterbury 2024-2025. Essentially, we're making a glorified GeoGuessr bot... More explained in the business section. This doc was mostly generated by MS Copilot, based on information from the Report and Report proposal (both of which are in /reports).

#### As the model is finalized this README.md will update.

<p align="center"> 
  <img src="./docs/gitembeds/header1.png" width="200" height="200" /> 
  <img src="./docs/gitembeds/thinking.gif" width="200" height="200" /> 
  <img src="./docs/gitembeds/springfield_w_point.jpg" width="200" height="200" /> 
</p> 
<p align="center"><em>Image Features -> NN Model -> Output Location</em></p>

## Table of Contents
- [Background](#installation)
- [Installation and Setup](#installation)
- [Usage](#usage)
- [Justiciations and Deep Learning](#DeepLearning)
- [Features](#features)
- [Contributing](#contributing)
- [License](#license)
- [Authors and Acknowledgments](#authors-and-acknowledgments)
- [Contact Information](#contact-information)

## Ever Played GeoGuessr?

GeoGuessr is an online geography game where players (solo or multiplayer) guess locations based on Google Street View images. The process involves analyzing visual clues such as road signs, landmarks, vegetation, and infrastructure to narrow down the location. Players then place a pin on the world map, earning points based on the accuracy of their guess.







## Installation

TODO

## Usage

TODO

## Features

TODO



## License

Information about the license under which the project is distributed.

## Authors and Acknowledgments

- **Noah King** - [GitHub Profile](https://github.com/2of)




## Geolocalization ? What is that? Locating Rocks?


Geolocation is the process of identifying the geographical location of a person or device using digital information such as GPS, IP addresses, or Wi-Fi positioning. It's widely used in various applications, from navigation and social media to fraud prevention and emergency services. 

[Learn more about geolocation and its applications here](https://www.esri.com/en-us/what-is-geolocation).


This repo uses only local image features to predict Latitude and Longitudinal Locations at city scale. While not a novel approach, the combination of explicit imgae features *is*. This is a model you can train at home! 


## What's in the repo?
Everything you need to build out your own GeoInformer is in here (or will be added) you'll need to BYO API key for google streetview. A few images are in /data for the purpose of validation


### Tools:
 1. A handy GUI tool for downloading google street view data (BYO api key) using QT5
<p align="center">
  <img src="./docs/gitembeds/tool_dler_gui.png" width="50%" />
</p>

 2. A handy tool to generate evenly distributed geolocation data for streetview images (as even as we can be given the obvious topologicial challenges
 (not shown as it's commandline, but here's the gist)
<p align="center">
  <img src="./docs/gitembeds/extraction_of_points.png" width="50%" />
</p>




# Background and Some Motivation
## Introduction

Most image formats contain geolocation information in their metadata or headers. When this information is absent, we propose that geolocation can be determined with useful accuracy from latent features derived directly from the image data itself. These features, referred to as Geo Informative Features, inform a deep learning model to determine latitude and longitude coordinates. Our data is sourced primarily from the Google Street View API.

The popular online video game GeoGuessr gamifies this process for humans. Players are presented with a randomized Google Street View location and score based on the topological distance between their selected location on a map and the actual capture location. This approach translates well to a machine learning model.

Some Geo Informative features are highly meaningful as they significantly reduce the candidate pool or are uniquely identifying (e.g., a sign reading "Carter’s Road Grocer, Carterton’s Top Grocer!" or a recognizable landmark like the Eiffel Tower). The most telling features are often derived from text, such as phone numbers and web domains.

![Simpsons Meme Placeholder](/docs/gitembeds/moespetshop.webp)


## Approach

We leverage several advanced techniques:

- **Object Detection**: Utilizing YOLO for efficient real-time object detection.
- **Text Extraction**: Using docTR for robust text extraction.
- **Color Extraction**: Implementing custom algorithms for HSV color histogram analysis.

These extracted features form a combined embedding. The embedding is then passed through a multihead attention layer, enhancing the model’s ability to learn complex patterns by selectively focusing on the most relevant parts of the combined embeddings. The architecture combines sparse features to form a dense embedding matrix.


Essentially, we want to concatenate **sparse** embeddings into a **dense** embedding, hereby creating something we call a **GeoLocalization Pattern**



![Simpsons Meme Placeholder](/docs/gitembeds/clowncar.gif)

Density is good, for this model

## Obtaining Data
Use the Graphical Tool to load a .csv of the candidate points which you wish to download.
Use point generator and point refiner (in ./tools) to generate said points
Define your root LAT/LONG in /config
Define your /(d/) in /config

## Recency

Where multiple Google StreetView Images are available at differing times for the same location, all images become part of the dataset.

## Preprocessing and Data

This GeoLocator is scoped to Chicago for computability, defining a 10km radius around the center point of Chicago. You can change this in ./CONFIG.py



The number of training points for a given radius is calculated as follows with integer rounding:

$$NumQueries = \frac{\pi \times R^2}{d^2}$$

Where \(d\) is the distance (meters) between each query point. The scaling of total candidate points about a radius is exponential. We define \(d = 10\), noting that StreetView images occupy some proportion of this location. The density of StreetView imagery is inconsistent across locales.

A critical notion in the test data is that local features used as geolocators should be present in multiple images. Defining \(d\) this way is critical to achieving good generalization of the model.

| Radius (km) | Number of Points (N) |
|-------------|-----------------------|
| 1           | 31,416                |
| 5           | 785,398               |
| 10          | 3,141,593             |
| 11          | 3,801,327             |
| 12          | 4,523,893             |


Select \(d\)  carefully! 


### Pipeline

The overall structure of our implementation is depicted below. The final model architecture is still being determined. The model has three branches:

- **Color Evaluation**:
  - Generates a histogram for the HSV color space.
  - Produces a corresponding color embedding.

- **OCR Text Extraction**:
  - Utilizes an OCR model to extract text (docTR).
  - Creates a text embedding from the extracted textual information.

- **Object Detection**:
  - Employs an object detection model trained on a manually selected pool of plausible objects.
  - Generates object embeddings for the detected items.

An attention layer then processes the combined embeddings:
- Dynamically weighs the importance of each feature.
- Enhances the overall geoinformative power of the model.

*Figure 7: Overall structure of the implementation pipeline.*

### Architecture and Activation Functions

Our model processes 640 × 640 input images using YOLOv11 for object detection and docTR for text recognition.

The input tensor size is (B, 640, 640, 3).

YOLOv11 outputs a tensor (B, N, 4 + 1 + C), where N is the number of detections per image, 4 represents the bounding box coordinates [x1, x2, y1, y2], 1 is the objectness score (Confidence), and C is the number of class probabilities.

docTR outputs a tensor (B, T, d), where T is the number of text instances detected, and d is the dimension of the text features.

The YOLOv11 embeddings are reshaped to (B, N, Ey) and the docTR embeddings to (B, T, Ed).

The combined tensor size before the attention model is (B, N + T, E), where E is the unified embedding dimension.

Given that the model exceeds 7 layers, we use activation functions like ReLU or Leaky ReLU to mitigate gradient loss, ensuring effective learning.

### Color Histogram Embedding

The overall color space of an image is a telling geolocator that permeates across input variables. The roofline in Figure 9, for example, is a telling geolocator, yet the sky is consistent but is not a useful feature for our model.

This branch of the model is essentially preprocessing only; we create color histograms representing the distributions of the local colors.

We convert the image to the HSV color space to better capture color variations and compute a histogram for each channel (Hue, Saturation, and Value). These histograms are then flattened and normalized to create the color vector.

The extracted histogram is used as an input feature in a separate branch of the neural network. This branch processes the histogram through several dense layers to identify significant color patterns. The output from this branch is concatenated with the object embeddings and the text embeddings before the attention layer(s).

*Figure 8: Example of HSV extraction from Figure 9 and a sample embedding (truncated).*

### Text Extraction and Processing

Text is a significant geolocator. We employ an off-the-shelf solution: the docTR OCR model. No hyperparameter tuning or model refinement is applied to this branch of the model. Images are normalized before feeding into the model.

Each training set member undergoes processing via the docTR model, which yields an object containing the localized text. While docTR may not capture every piece of text flawlessly, any errors introduced are minor and do not significantly impact the performance of the greater model.






